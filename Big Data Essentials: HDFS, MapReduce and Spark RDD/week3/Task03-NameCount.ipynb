{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, content = line.strip().split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    words = re.split('\\W*\\s+\\W*', content,flags=re.UNICODE)\n",
    "    for word in words:\n",
    "        sys.stderr.write(f'reporter:counter:Wiki stats,Total words,1\\n')\n",
    "        if ((word[1:].islower()) and (word[0].isupper()) and (not word[0].isdigit())):\n",
    "            print(f'{word.lower()}\\t1\\t1')\n",
    "        else:\n",
    "            print(f'{word.lower()}\\t0\\t1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "\n",
    "import sys\n",
    "\n",
    "current_key = None\n",
    "total_amt = 0\n",
    "total_tc = 0\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        key, count, total_count = line.strip().split('\\t', 2)\n",
    "        count = int(count)\n",
    "        total_count = int(total_count)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    if current_key != key:\n",
    "        if current_key:\n",
    "            print(f'{current_key}\\t{total_amt}\\t{total_tc}')\n",
    "        total_amt = 0\n",
    "        total_tc = 0\n",
    "        current_key = key\n",
    "    total_amt += count\n",
    "    total_tc += total_count\n",
    "\n",
    "if current_key:\n",
    "    print(f'{current_key}\\t{total_amt}\\t{total_tc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        word, amt , t_amt = line.strip().split('\\t',2)\n",
    "        amt = int(amt)\n",
    "        t_amt = int(t_amt)\n",
    "\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    if float(amt)/float(t_amt) > 0.995:\n",
    "        print(f'{amt}\\t{word}')\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        amt, word = line.strip().split('\\t',1)\n",
    "        amt = int(amt)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    \n",
    "    print(f'{word}\\t{amt}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `assignment*': No such file or directory\n",
      "20/09/19 13:58:12 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/09/19 13:58:16 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/09/19 13:58:16 ERROR streaming.StreamJob: Error Launching job : Permission denied: user=jovyan, access=EXECUTE, inode=\"/tmp/hadoop-yarn\":root:supergroup:drwxrwx---\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:350)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:311)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:238)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:541)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1705)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1723)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:642)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:110)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:2971)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1160)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:880)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n",
      "\n",
      "Streaming Command Failed!\n",
      "rm: `final_result*': No such file or directory\n",
      "20/09/19 13:58:29 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/09/19 13:58:29 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/09/19 13:58:30 ERROR streaming.StreamJob: Error Launching job : Permission denied: user=jovyan, access=EXECUTE, inode=\"/tmp/hadoop-yarn\":root:supergroup:drwxrwx---\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:350)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:311)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:238)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:541)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1705)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1723)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:642)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:110)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:2971)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1160)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:880)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n",
      "\n",
      "Streaming Command Failed!\n",
      "cat: `final_result/part-00000': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUT_DIR=\"assignment1_\"$(date +\"%s%6N\")\n",
    "HADOOP_JAR=\"/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar\"\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash assignment* > /dev/null\n",
    "\n",
    "# Code for your first job\n",
    "yarn jar ${HADOOP_JAR} \\\n",
    "    -D mapreduce.job.name=\"First Job\" \\\n",
    "    -D mapreduce.job.reduces=4 \\\n",
    "    -files mapper1.py,reducer1.py \\\n",
    "    -mapper \"python3 mapper1.py\" \\\n",
    "    -combiner \"python3 reducer1.py\" \\\n",
    "    -reducer \"python3 reducer1.py\" \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output ${OUT_DIR} > /dev/null\n",
    "\n",
    "OUT_DIR2=\"final_result\"\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash final_result* > /dev/null\n",
    "\n",
    "# Code for your second job\n",
    "yarn jar ${HADOOP_JAR} \\\n",
    "    -D mapreduce.job.name=\"Second Job\" \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.map.output.key.field.separator=\\t \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1nr\" \\\n",
    "    -files mapper2.py,reducer2.py \\\n",
    "    -mapper \"python3 mapper2.py\" \\\n",
    "    -reducer \"python3 reducer2.py\" \\\n",
    "    -input ${OUT_DIR} \\\n",
    "    -output ${OUT_DIR2} > /dev/null\n",
    "\n",
    "\n",
    "\n",
    "hdfs dfs -cat ${OUT_DIR2}/part-00000 | sed -n '5p'\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
